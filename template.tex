\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\title{Structure Learning to Solve the Delusion Paradox}

% Here you can change the date presented in the paper title
%\date{September 9, 1985}
% Or remove it
%\date{}

\newif\ifuniqueAffiliation
% Comment to use multiple affiliations variant of author block 
%\uniqueAffiliationtrue

\ifuniqueAffiliation % Standard variant of author block
\author{ \href{https://orcid.org/0009-0002-8002-3191}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Adam Manoogian}\thanks{These authors contributed equally to this work} \\
    Monash Centre for Consciousness and Contemplative Studies\\
	Turner Institute for Brain and Mental Health\\
	Monash University\\
	Clayton, VIC, Australia \\
	\texttt{adam.manoogian@monash.edu} \\
	%% examples of more authors
	\And
	\href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Elias D.~Striatum} \\
	Department of Electrical Engineering\\
	Mount-Sheikh University\\
	Santa Narimana, Levand \\
	\texttt{stariate@ee.mount-sheikh.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\else
% Multiple affiliations variant of author block
\usepackage{authblk}
\renewcommand\Authfont{\bfseries}
\setlength{\affilsep}{0em}
% box is needed for correct spacing with authblk
\newbox{\orcid}\sbox{\orcid}{\includegraphics[scale=0.06]{orcid.pdf}} 
\author[1,2]{%
	\href{https://orcid.org/0000-0000-0000-0000}{\usebox{\orcid}\hspace{1mm}Adam Manoogian\thanks{\texttt{adam.manoogian@monash.edu}}}%
}
\author[2]{%
	\href{https://orcid.org/0000-0000-0000-0000}{\usebox{\orcid}\hspace{1mm}Adeel  Razi\thanks{\texttt{adeel.razi@monash.edu}}}%
}

\author[1]{%
	\href{https://orcid.org/0000-0000-0000-0000}{\usebox{\orcid}\hspace{1mm}Jakob  Hohwy\thanks{\texttt{jakob.hohwy@monash.edu}}}%
}

\affil[1]{Monash Centre for Consciousness and Contemplative Studies, Monash University, Melbourne, Australia}
\affil[2]{Monash Biomedical Imaging, Monash University, Melbourne, Australia}
\fi

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Probabilistic models have garnered extensive interest as a method of characterizing behaviour and neural data in psychiatric disorders, grounding the field of Computational Psychiatry. However, the traditional structure of hierarchical inference in computational models commonly has one interpretation of noise - noise moves up through the hierarchy.which is filtered up through the hierarchy. Here, we will highlight the inability of this kind of model to capture environments in which noise would relate distinctly to individual levels of the hierarchy. We exemplify this problem with the delusion paradox, conceived as a pathological learning tendency that is difficult to explain under common hierarchical inference models. We consider specific learning biases and how they are captured (or overlooked) in models. Finally, we propose a modeling structure that can be cognizant of learning biases through a structured approach to inductive biases used to conduct inference. This proposal synthesizes hierarchical approach to modelling that is inclusive to how agents learn the structure of an environment. We discuss how this proposed structure may connect to other modeling fields in psychiatry, including associative networks and biophysical networks.
\end{abstract}


% keywords can be removed
\keywords{decision-making \and schizophrenia \and predictive processing \and computational models}

\section{Introduction}

In the field of Computational Psychiatry, probabilistic models, in which a hypothesis is tied to a normative learning rule to predict expected responses, provide a more  understanding of psychiatric symptoms, beyond what descriptive models can provide \citep{vrizzi2023comparing, haines2023from}. This approach incorporates neurobiology, behavior, and interactions with an environment to parameterize an agent's internal model \citep{adams2016computational}. The structure of the internal model consists of prior beliefs and learning parameters, each with associated precisions, the interactions allowed between possible states, and the resulting expectations. Deciding which set of these parameters and the state space is required for any given task involves a hand-crafted approach, which means taking on assumptions that may not be reflected in the system. 

A comprehensive approach, in which the entire range of possible priors, learning rules and state spaces is considered is not feasible for the complex systems that Computational Psychiatry attempts to encapsulate. Unconstrained sweeps through parameter spaces are often intractable, and can result in combinations of parameters that result in indistinguishable outputs, rendering their interpretation difficult \citep{gershman2016empirical}. Often, hypothesis-driven approaches can draw on the fact (based on the complete classes theorem) that pathological populations can be described as having different priors, which can be disentangled through careful design. However, this approach becomes more difficult as tasks complexify to capture more ecological behaviours, or where subject variability is higher due to heterogeneity of a disease.

The generative model acts as the structure of latent causes, or architecture of a model, over which inference is conducted. In practice, the structure of the generative model is typically assumed to follow the structure of the generative process (the causal factors and sequential, chronological steps of trial-to-trial). This simplifying assumption does not include many psychological factors people are known to carry into a wide array of learning, such as biases, heuristics and resource constraints \citep{lieder2020resource-rational}. This assumption is counterproductive if the aim is to emulate the generative model an agent actually uses, which may often deviate from the generative process. This issue is particularly relevant for schizophrenia, where an assumption of rationality and veridical representation as a good baseline for modeling behaviour must be questioned \citep{bari2024resource-rational}. By assuming structure as constant, structure learning deficiencies can be overlooked, which is significant since symptoms in schizophrenia often appear related to  an inability to infer the structure of the task.

Here, we formalize the argument of structure learning, which considers the inclusion of biases, heuristics and resource constraints, as it relates to common frameworks in computational models of schizophrenia. Rational models include reward and inference based models such as reinforcement learning, and the Hierarchical Gaussian Filter \citep{sutton1998reward, mathys2011bayesian}. They also include reduced Bayesian models, such as changepoint detection (CPD), and full Bayesian models that incorporate action into belief formation, such as active inference. Active inference is of particular interest, since it is a fully rational model which optimizes goal-directed learning under a singular cost function, inclusive of expected future outcomes.

Each of these modelling frameworks require design choices to be made by the researcher. Namely, an inference algorithm, a reward function, and an expected transition function. One could then construe symptoms relating to any of these requirements. Patients with the (admittedly broad) classification of schizophrenia have problems with inference, reflected as incorrect precision over the likelihood of information. Volition and negative symptoms have been found to alter reward-related behaviours, which would affect the ability to construct a generative model that assumes a reward function (or goal-state) \citep{culbreth2023transdiagnostic}; similarly, the beliefs over how states will change could be applied to uncertainty over the environment, which would implicate the transition function \citep{kaplan2016estimating}. In active inference, multiple combinations of these modelling choices can lead to parameters reflecting the behaviour of interest \citep{sajid2020degeneracy}, a computational degeneracy which tracks with previous descriptions of schizophrenia \citep{friston2016dysconnection}. However, the complexity of the parameter space also limits interpretation and model fitting to data, necessitating constrained models.

Contrary to the hypothesis-driven approach highlighted previously, data driven approaches use supervised or unsupervised learning, for which parameters are not immediately interpretable \citep{doerig2023neuroconnectionist}. Instead, the connectionist approach  avoids the problem of hand-specifying models by "letting structure emerge" \citep{mclelland2010letting}. Prominent examples of this approach within schizophrenia have included neural network and associative network models \citep{hoffman2001neural, stein2021biologically}.

Artificial neural network, biophysical network and dynamical systems models of schizophrenia could better act as the basis for meta-level inductive biases. Biophysical models constrain the generative model available for use through some form of dimensionality reduction, for which an active inference model can conduct free energy reduction. These two processes act in a reverberating sequence. Generative model production informs an active agent’s available actions, which initiates interactions with the environment, informing belief updating. Then, these parameters are compared against the most likely generative models that can produce them, and the process repeats. This process avoids the difficulty of constructing a hand-crafted state-space by letting a data driven process decide. %This approach resembles meta-learning \citep{xu2018meta, luketina2022meta}. An outer loop, or upper level model tunes the hyperpriors used by the lower model (the inner loop), which incorporates these hyperpriors to conduct inference over a task. In a Bayesian approach, this would be displacing the responsibility of priors to a higher level. 

We will exemplify how these modelling choices apply for a particular case, namely delusions, understood in their clinical sense as fixed beliefs that are not amenable to change in light of conflicting evidence. The $delusion\ paradox$ reflects that delusions are marked by both weakly held beliefs (allowing the often bizarre content to be believed) and strongly held beliefs (preventing belief revision) \citep{petrovic2023resolving}. Delusions, seemingly resistant and incorruptible beliefs, would be represented computationally as precise priors resistant to sensory evidence. Seemingly paradoxically, the other prominent description of beliefs in schizophrenia, aberrant salience, describes an imprecise internal model, highly susceptible to sensory evidence \citep{kapur2003psychosis}. Hierarchical inference models have not been able to explain how these computational phenotypes of strongly and weakly held beliefs can pathologically develop and entrench each other. We will suggest that the delusion paradox can be addressed by approaching these two computational phenotypes, previously described as 'all-or-nothing' learning \cite{nassar2021all}, as a structured switch between priors, or models, that an agent has access to. Similar computational approaches enable switches between learning strategies, responding behaviours or networks in the brain \citep{kolling2024role, cole2024cognitive}. 
%With the combination of the probabilistic and neural network models described, one can better expose the development of learning and perceptual changes underlying delusions and psychosis more generally.

Here, with a case study in the delusion paradox in schizophrenia, we highlight the need for a structured approach to inductive biases during the setup of an active inference model. We synthesize disparate findings from predictive coding, neural network, and Bayesian inference approaches as constraints in one language - state space models. With these methods, we can better theorize how constraints can inform the structure of our generative model of delusions. Motivating constraints will also help us understand how delusions form and foment. \ref{delusion_paradox} will elaborate the delusion paradox as described in predictive processing, and particularly the development of delusions. \ref{measuring_stability} will discuss the differences between inferential models in answering this paradox, and why their current formulations aren't suited to account for evolving states spaces. \ref{problems_learning} highlights learning abnormalities in schizophrenia that may underlie relevant belief updating. \ref{state_inf} notes the importance of structure learning and how it coincides with state inference. \ref{structure_learning} emphasizes how previous models can empirically motivate how to apply structure learning.

\section{The Delusion Paradox}\label{delusion_paradox}

With predictive coding and Bayesian inference models, (mis)-perception is understood as the effect of prior beliefs and their updating on sensory stimuli \citep{hohwy2013predictive}. The utility  of Bayesian inference for describing schizophrenia has been widely acknowledged, at both a descriptive 
\citep{Hemsley1986formation, bongiorno2025delusions, sterzer2018predictive, corlett2019hallucinations, sterzer2019decision, notredame2014what} and interpretive level \citep{horga2019integrative, horga2014deficits}. The description of schizophrenia in terms of inference also enables the simulation and fitting of models that encapsulate these theories \citep{ashinoff2022rethinking, adams2013computational, gibbs-dean2023belief, fromm2023belief, katthagen2018modeling}. Computational models allow a more explicit exploration of predictive coding, for which we can ascribe parameters to symptoms, neural data, and behavior \citep{friston2023computational, adams2017computational}, particularly for delusions \citep{corlett2010towards}. The canonical predictive coding account of psychosis describes positive symptoms, such as hallucinations and delusions, in schizophrenia. It does so through a general purpose, hierarchical model for which higher levels prescribe a decreased precision in low level prior beliefs and an increased reliance (and precision) on sensory data at the lower level \citep{sterzer2018predictive}. An alternative account, in which hallucinations result from an overreliance on highly precise priors \citep{corlett2019hallucinations}. Indeed, the frequency of both accounts seems about equal \citep{goodwin2023predictive}. 

%delusion definition
Delusions are beliefs defined by their rigidity and high certainty. The delusion paradox describes the existence of "fixed beliefs are not amenable to change in light of conflicting evidence" \citep{APA2013}. In computational studies, delusions are frequently described from the APA definition as "a false belief based on incorrect inference about external reality that is firmly sustained despite ... evidence to the contrary" . In Bayesian terms, this would be reflected as a highly precise belief. Given the complexity and content of delusions, it is plausible to consider these as high-level beliefs. Therefore, for computational purposes, we hypothesize the computational mechanisms which could support development of delusions as the solidification, constraint, or strengthening of high level priors.

%theories that explain delusions
A prominent theory of psychosis, aberrant salience, has been used to describe the transitions into delusions through a time period called the 'delusional mood'. Due to consistent unreliability of the internal model, often coupled with aberrant encoding of precision weighted prediction errors, a delusional belief could be allowed to remain as explanatory \citep{kapur2003psychosis}. Computationally, when aberrant salience (weak low, level priors) co-exists with the tendency  strong, high level priors, delusional behaviours are subsequently formed \citep{adams2022everything}. In a similar explanation, prediction errors formed during spontaneous thoughts could be produced despite a mismatch occurring, but due the difficulty of accommodating these in a good model, their error propagates upwards, creating high level, delusional beliefs \citep{fletcher2009perceiving}. Other explanations challenge the Bayesian architecture by questioning whether prediction error is actually registered, allowing mismatch between prediction and sensory feedback to be registered \citep{parrot2021delusional}. 

%problem 1 = 'where in the hierarchy' 
Therein exposes a weakness of predictive coding explanations; the level along a hierarchy on which inference occurs is difficult to determine. 
Instead, the inability of prediction errors to penetrate beliefs may lie in the identification of error to penetrate the relevant level of the hierarchy \citep{howhy2013predictive}. While high level priors are commonly thought to reflect abstract beliefs or thoughts, and low level priors perceptions, the boundaries between them are susceptible to interpretation \citep{goodwin2025predictive}. Although efforts to experimentally distinguish between levels between levels are common \citep{schmack2013delusions}. 
%problem 2 = interactions in the hierarchy%% 
In addition with identifying the level, it is difficult to know if, and how, which level acts as a 'compensatory' mechanism for the other \citep{petrovic2023resolving}. The causal, or directional, mechanism underlying this sort of computational dysconnection is theorized but experimentally underexplored in schizophrenia, and predictive processing frameworks do not provide an obvious solution \citep{jardri2013circular, friston2016dysconnection, adams2022everything}. A succinct account of the necessary components of delusion formation and maintenance must include interaction between low and high level uncertainties, and likely uncertainty throughout the hierarchy \citep{broyd2017dopamine}.

%temporal necessity of delusions
In their current state, traditional inference algorithms are not suited to explain the development of delusions, prompting the inclusion of heuristic mechanisms \citep{harding2024new}. Several mechanisms have been posited to explain the temporal evolution of delusions, such as the development of asymmetric learning rates, attractor dynamics or interactions with memory. We will elaborate on biases in learning and perception that exposes the problematic dynamics of delusions.

\section{Measuring the Stability of Beliefs}\label{measuring_stability}

%introduce state space models
While probabilistic decision making frameworks are common, the specifics in modelling frameworks measuring schizophrenia are many. Linear Gaussian systems can be used to unify many latent variable models used in Computational Psychiatry, including state space models, with a few assumptions. Here, the state-space model is requires only a transition model $p(x_{t+1} | x_t$ and observation model $p(y|x)$. The two models and their parameters are assumed to be time invariant, the dynamics are assumed to be linear, and states distributions as gaussian. These form the generative model, which follows the principles of predictive coding, meaning these mechanisms enable a structured prediction of the environment.

%more on defining the model
State space models are used to solve the filtering problem, the estimation of hidden states from observations. We begin a state space model by dictating how it will transition over time. Filtering involves the estimation of states given a sequence of observations. A set of equations describes the evolution of a state, or latent variable, through timepoints. The transition model indicates the update in beliefs from one state to the next, beginning from the initial condition. The observation equation serves as the perceptual model, indicating the transformation of that belief into an expected observation or action, or the likelihood of how a latent variable forms the output.

\begin{equation}\label{ssm}
The \ observation \ equation : p(y|x)                             \\
The \ transition \ equation:  p(x_{t+1} | x_t)
The \ initial \ condition: p(x_t)
\end{equation}

Bayesian models reflect behaviour through time varying learning that is reflected through the precision of different levels of beliefs \citep{preuschoff2007adding, mathys2011bayesian}. The advantage of the HGF is that it also encompasses nonlinear systems, as it adapts its volatility estimate accordingly to incoming data \citep{mathys2014uncertainty}. However, in quickly changing environments, normative models with hazard rates for 'changepoints' in the environment (changepoint detection models (CPD)), or volatile Kalman filters (VKF), potentially better describe the capability to quickly update beliefs \citep{nassar2010approximately, piray2020simple}. Other models of adaptive learning rates explicitly delineate volatility over different types of uncertainty \citep{payzan2011risk, yu2005uncertainty}.

We refer to a previous study for the full derivation of changepoint detection and the HGF in terms of state space models and Bayesian variational inference \citep{markovic2016comparative}. Extensions of the Kalman filter are also becoming used in this research domain. These include the VKF, which builds on state space formulations to include uncertainty-dependent learning rates \citep{piray2020simple}. The overarching theme of each of these models is that, for equivalent state spaces, uncertainty can be simply represented in very few equations. Precision over the observation and transition equations reflect the hidden state (belief given a state) and the learning rate (beliefs in how states will change). When put into this formulation, we can understand the difference between these models as differences in the configuration of hidden states (whether they are hierarchical or sequential) and learning rates (whether that learning rate is uncertainty dependent or an independent parameter).

\subsection{Inference Models in Schizophrenia}

Recall that the difficulty we have described in characterizing delusions is understanding the transition from weak to strong beliefs in the presence of imprecise internal models. The paradox of delusions can be summarized by the question- how can precise priors be learned from imprecisely inferred information?

%introduce problem of irrelevant info
Agents must be able to learn to distinguish between irrelevant and important information over time, in order to form useful beliefs about the world. As environments change over time, agents must be able to balance expectations over the utility of information, but also their expectations about how to expect whether information is helpful or not. For the purposes of understanding the development of delusions, we consider how beliefs are measured in dynamic, stochastic environments. Additionally, the state space must be sufficiently complex, allowing multiple options about how states will transition, ensuring a nuanced view of policy selection.

%illustrate the point that evidence is mixed
Models of dynamic belief updating in volatile decision making tasks can help probe maladaptive reasoning in psychosis. We refer to previous reviews for a thorough understanding of behavioural findings in schizophrenia with HGF and changepoint detection models \citep{ashinoff2022rethinking, katthagen2022models, gibbs-dean2023belief, goodwin2025predictive}. Commonly used is the Hierarchical Gaussian Filter \citep{mathys2011} and changepoint detection \citep{nassar2010}, in which volatility is monitored via a 'meta'-belief. In these sequential belief updating tasks, schizophrenia patients make bigger belief updates to unexpected information, yet reduced belief updates to expected or consistent information \citep{ashinoff2022}. Larger than optimal belief updates to unexpected information has been explained through various mechanisms in these models, including aberrant salience \citep{Adams2018} or mis-estimations of volatility \citep{cole2020}. Expectations about volatility are higher, resulting in excessive switching behaviours in expectation of underlying changes \citep{gibbs-dean2023belief}. Learning rate itself is often found to be lower in tasks, with a bias towards less exploitative decisions \citep{gibbs-dean2023belief}. Importantly, when learning rate differences were not found, a dependence on an 'all or nothing' updating strategy could be inferred from decisions (Nassar et al., 2021). Finally, there may be difficulty in understanding the underlying task structure, explaining this tendency for switching behaviours and potentially decision making strategy \citep{schlagenhauf2014striatal}. There is difficulty in tying the significant parameters in inferential models to the actual presence in delusions \citep{ashinoff2022rethinking}.

Active inference builds upon predictive coding conceptualizations to understand belief emergence as an active interaction with the environment. The inclusion of action in these models is a necessary next step, motivating our usage of this framework \citep{sterzer2018}. Beliefs change as one interacts with the environment; as perceptions go on to update beliefs, beliefs will fuel what future perceptions will be and the actions an agent takes to reach them. The uncertainty between them must evolve. Readers are referred to predictive coding \citep{millidge2022predictive}, and a full in-depth explanations to active inference \citep{dacosta2023reward, friston2016active}. We will briefly introduce the relevant methods of these inference models in evaluating data and forming beliefs that are both sequential and hierarchical, emphasizing the aspects of active inference models that are advantageous.

\subsection{Sequences in Inference Models}

When considering previous timepoints in Bayesian updating schemes, the previous state serves as a sufficient statistic (whereupon the posterior acts as the new prior), avoiding the necessity of recalculating the sequence of previous information. However, the efficacy of this simplification is lost when information about previous states must be changed. In this scenario, ‘one-step’ models (HGF, RW-RL and changepoint detection models) adjust precision over the current belief. Active inference extends the necessary change in precision to the entirety of the sequence of states. It does so in the future direction with expected free energy, which incorporates beliefs about how actions will change expected outcomes in the environment. Previous states along the sequence are also updated in regards to variational free energy and if future inputs change beliefs about past beliefs or states.

BOX:
Variational Free Energy is the expected difference between the approximate posterior and the generative model. Free energy is minimized when the approximate posterior becomes the true posterior, or when the model evidence is maximized for observed outcomes.
Additionally, the Expected Free Energy term arises in active inference when priors are placed over policies $pi$, or sequences of states controlled by actions, which in turn determine subsequent, unseen outcomes. This allows us to use policies to minimize the free energy of beliefs about future possible outcomes in relation to prior beliefs.

In active inference, agents select between policies based off their expectations about how actions take them through predicted states and how likely that trajectory is to take them to their preferred states. Policies are decided by previous beliefs (F or variational free energy), expected future beliefs controlled by the precision of those beliefs (G or expected free energy) and habits (E).
%box end

Learning under state space models can be described as more than just updating with an adaptive learning rate, if knowledge about state spaces are incorporated into assumptions \citep{mcnamee2017efficient}. The model-free versus model-based distinction in reinforcement learning is relevant here. HGF and CPD have learning rates akin to incremental learning, whereas model-based would define a full understanding of the state space and how to traverse through it with a policy of multiple steps. HGF does not explicitly look ahead into that state space, but captures the uncertainty of the environment (as a function of the past) and creates a prediction of the future based on only past observations.

In active inference, policies use a model-based approach to incorporate the past, present and future states, for which variational free energy is minimized to select between possible options. The policies must also incorporate how well that policy brings the agent in-line with their preferences. Active inference can be thought of as building on the Kalman filter to understand beliefs as they relate to previous and future timepoints. It can also be thought of as re-extending the changepoint detection model, which is reduced from an optimal Bayesian model. While the HGF can minimize parameters over the length of a sequence (T), it does not do so in an online fashion. Also in HGF, an agent's trajectory cannot be inferred beyond its current timestep, as optimisation of parameters on timesteps beyond this requires the observations at those timesteps.

\subsection{Inference models represent a hierarchy of beliefs}

Hierarchical predictive coding implies an overarching top-down belief which delineates the low level priors for inference. Bottom-up information refers to prediction errors that are decided by the difference between observations and low level priors, meaning that information at the top of a hierarchy is filtered by the bottom. A prevailing assumption of the hierarchical nature of the brain is that lower levels (of the model and brain area) track faster changes, and higher levels track more slowly changing statistics of the world \citep{kiebel2008hierarchy}. Few top-down beliefs should implicate many low-level priors for inference, in which levels lie on different temporal scales \citep{friston2018deep}. If the assumption that sensory evidence travels through a model from lower to higher levels is paired with this intuition, then it is incompatible with the nature of delusions. This incompatibility has only been explained away with a ‘compensatory mechanism’ (in part 1). Instead, the hierarchical framework needs nonlinear processing, context-dependent processing, or interactions with memory for delusions to form \citep{sterzer2018predictive}. Nonlinear hierarchical coupling may be necessary for linking levels with differently sized state spaces, and has been described in HGF \citep{weber2023generalized}.

The problem described is about 'where' in the hierarchy prediction error should be resolved. Uncertainty can be described up to four levels, including structural uncertainty, and most commonly has done so within three levels of HGF \citep{sandhu2023transdiagnostic}. The three levels of HGF represent an increasing tendency of volatility in the environment. As volatility is expected, it increases imprecision of beliefs of the environment, represented by imprecision up the hierarchy. Volatility, the speed of change of uncertainty in an environment, has been described as distinct from stochasticity, moment-to-moment changes in the environment \citep{piray2021Model}. Tracking the different types of uncertainty has been related to mental disorders, particularly anxiety \citep{pulcu2019MisestimationUncertaintyAffective, hedley2023UnderstandingAnxietyUncertainty}. The level of uncertainty can be thought to roughly correspond to increasingly abstract hierarchies in the brain. Computationally, the question in schizophrenia is where to apply that uncertainty, up to the level of structural uncertainty.

A higher volatility should beget a higher learning rate, so that an agent can more rapidly update their beliefs to changes in the environment. On the other hand, the more stochasticity there is in the environment, the lower learning rate should be to these observations, as they are uninformative to the mean of the underlying process. Models that are often used to empirically explore volatility include changepoint detection. Uniquely, expected free energy explicitly provides a value accounting for the uncertainty of future states. Policies incorporate both risk, perceptual uncertainty, and estimation uncertainty into the singular cost function, allowing the joint minimization of both in regards to beliefs about transitions over state spaces \citep{bruckner2024DecisionMakingUncertainty, bruckner2022UnderstandingLearningUncertainty}. Risk, or outcome variability, which is usually allocated to a separate response model or to stochasticity \citep{mathys2011BayesianFoundationIndividual}. The equivalent of perceptual uncertainty in active inference is state estimation.


%start box
BOX:gm
In active inference, stochasticity would likely represent state likelihood (A matrix), while volatility the transitions between states (B matrix).
The state space formulation of volatility and stochasticity is as follows:
volatility as:
stochasticity as:
%end boxIn volatile environments, an agent's updating of their internal model is guided by the difference between their model and the evidence. The precision of their beliefs describes how evidence will be integrated into the current belief. 
In volatile decision making tasks, a higher order structure of the task environment is present. In these tasks, participants must form a prediction consisting of a mean and variance of a stimuli, and how often that stimuli can change. Observations that actually deviate from the prediction reflect an underlying volatility of the task, and are often referred to as 'changepoints'. Whereas observations that are only indicative of noise in the process are 'oddballs'. Blocks of time where noise in the observations represents oddballs or changepoints are often explicitly instructed to the participant, or are easy to determine between. Therefore, the agent knows in what fashion to immediately interpret noise. So they resort to their normal update strategies in interpreting noise, flexible or perseverative (corresponding to meanshift or oddball). Tasks in which the underlying changes in structure are more difficult to interpret enable a better understanding of learning that takes place when noise is difficult to interpret.

Surprise measures an attempt to capture where in the model that error originates from (and likely to lead to model updates); they all can be captured entirely by belief \citep{modirshanechi2022taxonomy}. In this manner, expected uncertainty describes the stochasticity of future outcomes that are attributed to the internal model's understanding of the statistics of the environment (or task). Unexpected uncertainty therefore occurs in response to observations not accounted for by the model. In many models and experimental setups, the difference between the uncertainties described are difficult to distinguish between.

\subsection{Dissociating Volatility}

What isn't possible through these types of models is the following scenario. Weak low level priors taking in observations, and having uncertainty over the error ascribed by those observations. But then that uncertainty should not translate upwards into a precise belief at the higher level. So, with these models, we cannot have the development of high level strong priors while still having the existence of low level weak priors (that never develop into strong). The comparison between the predictive coding description and inference models (that use uncertainty to describe beliefs) should be made explicit here. In the same manner that fixed, high level beliefs cannot be formed from uncertain low level beliefs, certainty about the volatility in the environment cannot be formed if beliefs about stochasticity are high. This distinction describes an inability to use the prescribed updated mechanisms to explain high level precise beliefs (about uncertainty). This implies that the assumptions we use to define volatility (and therefore dissociate the level of a hierarchy) may be used incorrectly. Here, we review empirical evidence that supports these hypotheses as they relate to behavioural data in schizophrenia.

Static environments can be used to estimate priors before they are used in inference. Beliefs in paranormal phenomena correlate with an inability to discriminate noise from signal in perceptual tasks \citep{seymour2022believing}. When assessing the strength of low level perceptual priors in natural scene statistics, such as brightness, line length and motion, there is a lack of evidence for abnormal priors at this level \citep{kaliuzhna2019no}. This indicates that higher levels of the predictive coding hierarchy must harbour impairments, or that impairments may instead arise when information needs to be integrated. In the case of perceptual organization, low level responses are indeed contextually modulated \citep{seymour2013altered}. There lies reduced susceptibility to visual illusions in schizophrenia, but most of these illusions are more complex tasks, such as the Hollow Mask illusion, instead of basic visual information \citep{notredame2014what}. Explicit risk tasks also offer a proxy to pre-set priors dictating behaviour because they also involve tasks without learning \citep{purcel2022review}. In these tasks, it may be more likely that risk imperception, or a reduced ability to discriminate between choice riskiness, accounts for problematic decision making in schizophrenia. Computationally, this would refer to priors over policies being more similar, or less ability to distinguish between them. Priors to consider gambling are higher at the beginning of a task \citep{kirschner2024transdiagnostic}.

A summary of HGF and CPD models concluded that behavioral performance deficits stem from an inability to determine whether volatility pertains to contextual (environmental) shifts or irreducible noise \citep{katthagen2022models}. Determination of where in a hierarchy to apply prediction error can be placed as a two source, one signal problem (commonly referred to as a system identification problem in engineering). From only one observation, the agent must determine how much to allocate that signal to prediction error at a perceptual or cognitive level. Unfortunately, there is not a definitive normative solution to this problem, and there are outstanding issues on the ability to distinguish uncertainty into its subtypes.

System identification can be measured in detection tasks, where observers must distinguish between noise and stimuli \citep{rahnev2018suboptimality}. Equally rewarded and presented stimuli differing only in variability results in observers choosing the less noisy stimuli more often. Structured techniques in distinguishing between state versus model updating with regard to prediction error have been conducted \citep{rutar2023differentiating}, and can used to disssociate behaviours \citep{oreilly2013dissociable}. Prediction error of the observation could be contrasted against prediction error of the model \citep{rutar2023differentiating}.

In dynamic belief updating models, an inability to distinguish between stochasticity and volatility is linked to reduced behavioural performances \citep{katthagen2022models}. It is thought that there may be biased tracking of only high volatility information \citep{katthagen2022models}. Different types of uncertainty can coexist, with low level precisions but also increased high level uncertainty being found in clinical high risk patients \citep{cole2020atypical}. In line with this, there is a difficulty in determining the relevance of events, as supported with models’ best fit with a bias towards irrelevant evidence \citep{kesby2023neural}. Despite no differences in behavioural results, SZ patients still reflect higher uncertainty \citep{kreis2022spared} and often overallocated the effect of noise to contextual inference \citep{kaplan2016estimating}.

There have been efforts to disambiguate stochasticity and volatility through joint estimation of these effects \citep{piray2021model}. Disambiguating volatility and structure over time is also an ongoing question \citep{yu2021adaptive}. Behaviourally, the only evidence to distinguish between these two types of uncertainty is the autocorrelation of choices. There has been efforts to disentangle beliefs over environmental volatility and outcome volatility, allowing them to vary independently of each other \citep{mikus2023computational}.

Notably, there is difficulty capturing structural uncertainty in schizophrenia. When accounting for decision making in an HGF model, SZ patients show an increased tendency of environmental volatility accounts for increased choice switching behaviors \citep{deserno2020volatility}. This effect included increased susceptibility to irrelevant losses. The authors also suggest that sensory inputs are overly stable, indicating strong perceptual priors in line with \citep{powers2017pavlovian}.

Disruption may occur at different levels of a perceptual-inferential hierarchy. Low level processes are allocated to more detailed, or visual processes. High level processes are thought to be more abstract, categorical and cognitive. It is difficult to interpret the strength of priors as following the canonical predictive coding account, because a determination on whether a task is more cognitive or perceptual is difficult \citep{stuke2019delusion, schmack2017enhanced, wengler2020distinct}. Indeed, high level (cognitive), strong priors may be a compensatory mechanism for initially weak, low level (perceptual) priors in delusions \citep{schmack2013delusion, schmack2017enhanced}.

Neurally, different types of uncertainty have been decomposed in fMRI BOLD activity \citep{mcguire2014functionally}. These two types of uncertainty can be broken down into sensory uncertainty and environmental uncertainty. When experimentally manipulating for both types of uncertainty \citep{fritsch2023sensory}.Confidence has proved useful in determining the precision associated with beliefs in probabalistic reversal learning tasks. In the case of 2nd order volatility, NMDAr inhibition implicates the fronto-parietal network [@vinckierConfidencePsychosisNeurocomputational2016]. Future studies can assess how this confidence is measured under the multiple types of volatility described. 

The volatility versus stochasticity problem has also been described through "signal-to-noise discrimination" or "source discrimination" in schizophrenia [@bellExaminingRelationshipsCognition2024]. The self-recognition account of hallucinations prescribes an alienation of self-generated events, as well as a misattribution of self-generated events to another origin. Schizophrenia has been theorized as a noise processing disorder [@keshavanConceptualizingPsychosisInformation2022]. There also lies a large similarity in internally representations and representations tied to external stimuli, indicating that dysfunction of either could affect learning and possibly contribute to hallucinations [@kohUsingInternalMemory2020] [@griffinPredictiveProcessingSource2017] . Movement tasks in which sensory attenuation is measured after external force are used to discriminate the reliance on endegenous or external signals [@brownActiveInferenceSensory].

The ability to distinguish volatility may interact with modality, particularly in schizophrenia. Hallucinations are more often more complex in the audio-verbal domain, indicating a modality dependent difference in sensory information. The contextual integration of volatility can be expanded to multi sensory problems in schizophrenia, such as the binding problem. The differentiation between causal models is subject dependent [@kordingCausalInferenceMultisensory2007]. By constraining the ability of volatility estimation through neuromodulators and resource limitations, we can better understand how the source of signal is identified by the system. Theoretical work will need to describe the mechanisms on which measurable effects could affect system identification and structure learning.

In Part 2, we have highlighted how prominent modelling frameworks use differing levels of volatility or hazard rates to capture changes in uncertainty. However, these models do have some shortcomings in the understanding information through the entirety of a sequence, as commonly measured by the trajectory of a belief. Active inference models offer the expected free energy term, which extends the trajectory of the belief into future possible observations. Second, there is an inability to describe the development of delusions with current modelling frameworks as the bottom-up normative description 'where in the hierarchy' sensory prediction error disrupts is lacking. Where the uncertainty is applied hierarchically is also a problem, as shown in difficulty determining between stochasticity and volatility. We turn to common irrational behaviours, or biases, in schizophrenia in order to evaluate the potential of these missing explanations under this lens.

BOX:
Epistemic uncertainty is uncertainty due to lack of knowledge, and therefore is reduceable if the agent obtains more information. Aleatoric uncertainty is irreducible, and refers to the inherent randomness of information and the environment. Epistemic uncertainty can be further broken down into parametric and structural uncertainty [@hedleyUnderstandingAnxietyUncertainty]. This delineation of uncertainty is similar to expected, unexpected, and higher order uncertainty [@pulcuMisestimationUncertaintyAffective2019]. Here, unexpected and higher order uncertainty likely correspond to parametric and structural uncertainty (within epistemic uncertainty). Expected uncertainty corresponds to aleatoric uncertainty.
Expected Uncertainty = [@soltaniAdaptiveLearningExpected2019] Expected uncertainty is uncertainty in reward outcome attributable to the probabilistic nature of the task. It is thought to reflect variability that is unavoidable, and should not be learned upon as it is already accounted for. Expected uncertainty does influence learning by allowing a baseline of acceptable variance.
Unexpected Uncertainty = effects arising from surprising 'jumps' (Yu & Dayan 2023)
[@soltaniAdaptiveLearningExpected2019] = Unexpected uncertainty is uncertainty that is not built into the model. However, it is unclear in delineating when changes in the environment should be considered predicted.
Volatility = Or defined as 'uncertainty due to changes in the generative process of the environment' [@soltaniAdaptiveLearningExpected2019]. This is captured by the hazard rate in changepoint detection formulations.
Stochasticity = Variability in the data unrelated to the underlying dynamics of the environment. This is often associated with outcome variability. When this quantity is known, it is sometimes referred to as risk.
While we can delineate outcomes into these two categories based on the mean of the data seen or the ground truth process, it is up to the agent to decide how to interpret this noisy information. The integration of these observations and the error associated with them is decided by the type of uncertainty it is attributed to. Thus, the categorization of new data as surprising is dependent on the model, and further dependent on where in the model the error is being compared to [@modirshanechiTaxonomySurpriseDefinitions2022].
Salience = attribution of evidence to current belief. "aberrant salience"

\section{Part 3: problems in learning in schizophrenia that may be latent in volatility estimation}\label{problems_learning}

Many of the biases outlined here come back to a failure to incorporate basic principles in human learning (Lake et al., 2017). We outline the relevance of a few learning deficits present in schizophrenia as evidence of under incorporation of principles in human learning. We introduce relevant cognitive biases, then reflect on how these may be instantiated computationally. The Bayesian modelling program does not inherently have an explicit answer for heuristics and biases and they must be hand built into the model.

Inherent in computational modeling of cognitive processes is the assumption that the structure of the model follows biological processes mimicking the generative process of the data. Policy compression, the simplification of policies to account for cognitive cost, has proven important for many behavioural phenomena, and exemplifies computations that can be overlooked in naive model construction [@laiPolicyCompressionInformation2021]. In the same manner, the hierarchical structure of a model can account for biases that may violate simple Bayesian inference (Sharp et al., 2022).

Many learning biases in schizophrenia are viewed as the following: jumping to conclusions or belief inflexibility . These again are seemingly paradoxical, characterized by decision making on less evidence than expected, and a inability to change beliefs in response to evidence, respectively. The jumping to conclusion (JTC) bias accounts for both decision making with less evidence as well as extreme responses, and the presence of this bias is suggestive of susceptibility to delusions [@dudleyPsychosisDelusionsJumping2016]. Whereas belief inflexibility is often characterized by perseverative behaviours or habits. If characterized by just learning rate, these biases would be represented by fast updating with a high learning rate, and following some shift, slow updating with a small learning rate. A belief against disconfirmatory evidence (BADE) reflects a devaluation of the importance of disconfirmatory evidence (evidence against the current hypothesis), which can be captured computationally as an asymmetric learning bias. BADE will transition from expected uncertainty to novel uncertainty as the environment is solved [@corlettModellingDelusionsTemporallyevolving2021] This requires a dynamic intake of evidence, a problem commonly conceptualized with learning rates [@behrensLearningValueInformation2007].

\subsection{Tasks (and beliefs) must be temporally dependent}

Sequential sampling tasks are necessary to understand the development of beliefs over time, as well as the interactions of parameters. We focus on sequential learning tasks as the development of parameters can only be disentangled through this lens. Variational techniques help extend beyond filtering and smoothing schemes, to understand how states and parameters transpire together [@fristonGeneralisedFiltering2010]. The development of delusions can therefore be characterized through its model that develops through interactions with the environment [@bramleyFormalizingNeurathShip2017] [@hohwySelfEvidencingBrain2016].

Inherent in this task is the necessity of understanding it not as a bandit, but as a sequential task in which the order and timing of events matters. The discrete trials of a task should not be viewed as discrete calculations, but on a trajectory within a state space, dynamics that have been described as continuodiscrete [@frolichNeuronalSequenceModels2021]. In this vein, a primacy bias, the overstated importance of information seen at the beginning of the task, has been found in schizophrenia. How that affects integration throughout the entirety of the following sequences of observations is unclear. The only way to explain this is through an effect of learning that is unexchangeable, as it is unlikely the actual capability of learning is so dynamic in this task. Learning rates have also been shown to be dynamic, with a reduction in the decay of learning rate upon repeated observations [@kirschnerTransdiagnosticInflexibleLearning2024].

Delusional beliefs may stem from an overreliance on initial information of a task. There is an order of information effect that affects the rigidity of information \citep{ashinoff2022}. This means that information does not have exchangeability, and equivalent data inferred in a different order can produce different results. The mechanism in which inferential processes drive or reduce information seeking is not entirely clear [@bakerDistinctInferentialMechanism2019]. However, the rigidity and certainty of delusions is determined by a primacy bias that depends on the sequence of available information.

Healthy participants also exhibit a primacy bias, shown on probabilistic inference tasks when switches between blocks of volatile and stable periods [@wallConsistencyKeyLearning2023]. Learning to ignore irrelevant information is unaffected in healthy participants, whereas updating to disconfirmatory information is dependent on the helpfulness of updating in early context blocks. The beads or urn task, in which participants sample information until stopping to make a decision, shows that schizophrenia patients respond faster, integrating information more quickly [@averbeckProbabilisticLearningInference2011]. However, when controlling for delusions, this effect is reversed, and complicated by a further bias towards initially sampled information, indicating that the order of information asymmetrically influences for belief formation [@bakerDistinctInferentialMechanism2019]. Opposite biases towards draws to information based on delusion and perceptual severity indicate that different biases could be co-occurring at different levels of the hierarchy, and could differ depending on the progression of the symptomology.

Similarly, an order-driven effect has been studied in relation to mismatch negativity in event-related potentials [@fitzgeraldMakingSenseMismatch2020]. For which, longer blocks of multiple context-blocks (switches between volatile and stable sequences, differing in deviance likelihood) exposes a learning effect (shown in MMN) dependent on the first context seen. A local learning rule is insufficient to explain this effect. The authors postulate a hierarchical effect of context can explain this effect through strong top-down priors.

Information integration can be calculated by a some form of recency [@nassarAllNothingBelief2021]. Part of this effect has been modelled as a reduction in the confidence of old information, resulting in an overweighted recency bias [@joyceExaminingBeliefConfidence2013]. Choice history biases are thought to be an adaptation to these statistics present in natural environments. In healthy participants, choice history biases are helpful. By exploiting the redundancy of information in sequential tasks due to the natural stability of environments. However, in tasks like RDK, which allows measurement of these biases through ambiguous stimuli, choice history biases are less influential in schizophrenia patients.

The temporal correlation of beliefs is often described as sequential effects, which are common in perceptual tasks [@rahnevSuboptimalityPerceptualDecision2018]. Positive autocorrelation, the most common effect, is the same response on current trials when compared to previous. Recency effect describes the overweighting of stimuli towards the end of a sequence, and of similar presentation [@rahnevSuboptimalityPerceptualDecision2018]. The repetition of action or bias in belief in these effects are commonly measured in schizophrenia behaviour [@eckertCrossModalityEvidenceReduced2023]
An understanding of temporal estimation, such as tracking a moving target, is also problematic in schizophrenia [@alustizaMetaanalysisFunctionalMagnetic2017]. While temporal dyscoordination is present in neural mechanisms in the brain, we focus on the effects behaviourally. In simulations of oculomotor motion patterns, sensory precision error has been shown to preclude strong prior beliefs (Adams et al. 2012, 2013, 2015. Evidence-accumulation rates for a time to collision task (requires tracking and predicting) are not different for SZ subjects [@limongiKnowingWhenStop2018]. Instead, high confidence in priors beliefs lead to problematic responding on the task. However, this study did not look at the progression of beliefs over trials, or within trials.

\subsection{Biases in learning rates prove more useful for models}

The parameters for inference models are time-invariant. At the end of a simulation, or fitting data, the parameter reflects the behaviour throughout the entirety of the sequence. These parameters are updated by the bottom-up processes, responding to outcomes in the world. While the top-down process is reflected in the prior, the efficacy of a prior at the beginning of the task goes away as more data is collected. A stronger top-down influence would be the continuous updating of the prior used based on endogenous processes. Computationally, this type of top-down influence would be reflected as updates on priors, or updates on the learning rate, with some independence of sensory processes. 

Attractor biases in learning rates fill the role of dynamic learning rates.
One method for capturing differential updating of learning rates is by separating confirmatory and disconfirmatory evidence [@adamsAttractorlikeDynamicsBelief2018]. By modifying an HGF model with parameters reflecting belief instability, decision-making under schizophrenia is better captured. This gets to the effect of time-varying parameters, in that beliefs are affected differently depending on how far they are away from a setpoint (so their ability to be update changes over time).

In active inference, inbuilt are point attractors in the equations as priors.  is a state prior that determines the change in motion of  [@parrGenerativeModelsSequential2023]. Habit, is one example that is built-in; it captures the perseveration over policies, or an increased bias to repeat actions. Previously, attractor dynamics have been described through habitual actions and actions taken by a schizophrenic agent, which in turn solidify the perceived expected state space of an agent [@adamsEverythingConnectedInference2022]. In increasing the precision over priors, the interaction between states under said policy is inevitably increased, as habitual states become more preferred [@adamsEverythingConnectedInference2022] This 'locking in' to prefer certain states itself acts as an attractor.

Also, the interaction between actions and the behavioural mode is an ongoing area of research. While hidden Markov models have been used to distinguish behavioural modes, active inference provides the necessary tools to understand the switches between behavioural modes. This leaves open the modelling question of how to account for externally attributed actions. Correlating with delusion severity, the bias towards a self-selecting hypothesis is exacerbated [@whitmanBiasFavourSelfselected2013]. The switching between internal and external modes with the internal mode representing a bias on the sequence of past trials and external mode more entrained to current stimuli [@weilnhammerSensoryProcessingHumans2023].

\subsection{state space compositionality}

One criticism of delusional beliefs is how they are relevant enough to even be in the hypothesis space, and once there, why are they not disregarded for more evidenced hypotheses [@parrottDelusionalPredictionsExplanations2021]. Biases in the availability of states (and therefore hypothesis space) perceived by the agent cannot be explored, because inference models specify a hand-crafted state space within which learning happens. An underlying problem is that patients as a whole have less model-based reinforcement learning, indicating a less extensive presence or usage of a model of the task and its transitions [@culbrethReducedModelbasedDecisionmaking20160512]. The susceptibility to conditioned hallucinations also implies a problematic pairing during integrations of beliefs, here in the form of different modalities [@powersPavlovianConditioningInduced2017].

The structure of the generative model is inherently causal, for which causality is interpretable if certain assumptions are met. Questions about causality can be expressed in hierarchical bayesian models [@goodmanLearningTheoryCausality2011]. Questions about compositionality follow, as the makeup of the generative model is defined by the components on which are available. The compositional makeup of a hierarchical generative model could include at what level prediction error from the environment is initially compared against. Traditionally, predictive coding models prescribe that error is filtered bottom-up through the model; but alternate theories could be built in models that can recompose their state space. Compositional inference underlies basic forms of perception, implying that the same principles should apply to goal-directed decision making [@zeithamovaHippocampusInferentialReasoning2012].

The final learning bias we assess in schizophrenia is an disruption in reasoning skills. Deductive reasoning involves inferring information based on existing knowledge, and can be thought of as reusing beliefs in novel ways in state space models [@moustafaDeductiveReasoningAbilities2021]. Teleologic thinking, the ascription of purpose to events, describes this phenomenon, alongside the attribution of paranoia and hallucinatory symptoms when evaluating low level stimuli [@obesoWolfSheepParanoid2024]. There is debate over whether reasoning processes or associative learning is disrupted [@ongchocoExcessiveTeleologicalThinking2023].

Empirical evidence for a lack of deductive reasoning stems to a few findings in transfer learning and memory. The ability to recombine relational information to make novel inferences is impaired in schizophrenia [@titoneTransitiveInferenceSchizophrenia2004]. However, other formal reasoning tasks such as syllogistic reasoning provide confusing results[@moustafaDeductiveReasoningAbilities2021]. The ability to intuit over misleading content (what would normally be irrelevant information) seems to be confounding results, along with cognitive impairments. In these misleading scenarios, where context would provide priors affecting logic in a 'common-sense manner', we see the prevalence of weak priors (on seemingly high level processes) [@selesnickQuantumlikeLogicsSchizophrenia2012]. Reasoning over associations is also disrupted by aberrant salience [@miasnikovaCrossfrequencyPhaseCoupling2021]. Also, previously described primacy biases could also be misinterpreted as reasoning biases [@bakerDistinctInferentialMechanism2019]. At the current stage, further experiments are needed to disentangle reasoning biases with associative learning.

\subsection{Interim conclusion on learning biases}

The discrimination of volatility into the varieties of uncertainty also hinders our ability to properly distinguish a hierarchical model. Many problems may have been hard to describe because of the muddled description of where inference belongs in the hierarchy. There are inherent dynamics in learning and decision making that expose the development of delusional behaviours within a task. The biases that form these dynamics are able to form in spite of our problem of hierarchical models. These are exposed through learning rates that are allowed to vary. In the next parts, we will explore how to motivate empirical approaches that explore how to best account for these biases.

\section{A role for state inference in schizophrenia}\label{state_inf}

Here, we highlight advances in modeling that do not hold the assumptions that Bayesian models build from, and emphasize where they could be helpful in better describing behavioural problems and symptoms in schizophrenia. We mainly focus on delusions but highlight the ability to incorporate these in hallucinations and disorganization. These methods are necessary to provide a structured approach to meta-learning throughout a task, in order to probe the learning biases discussed. The learning biases may fall out of the assumptions of bayesian models: time invariant parameters and rigid state spaces.

Time invariant parameters can be explored by models that assess the entirety of a sequence (its trajectory previous and forward to the current state). While bayesian filtering captures updates of beliefs about the current state, sequential inference has been used to describe updates to both past and current beliefs, in a manner extending the concept of autoregressive models to inference [@fitzgeraldSequentialInferenceMode2017].

The inclusion of complex state spaces, or cognitive maps, into the linear gaussian state space model formulation can also extend analysis to the complex state spaces needed to disentangle structure learning. These are more flexible for detailing complex environments [@whittingtonTolmanEichenbaumMachineUnifying2020; @behrensWhatCognitiveMap2018]. The cognitive map is thought to extend beyond locations and into cognitive processes as well [@schuckSequentialReplayNonspatial2019d]. One approach is the CSCG, which creates new states [@rajuSpaceLatentSequence2022].

\subsection{State Inference}

A potential approach that works within Bayesian concepts is non-parametric bayesian statistics, which offload specification of number of states in a model to an adjusteable parameter[@gershmanContextLearningExtinction20100111] . In computational psychiatry, the state inference is usually allocated to the role of 'context', which contains the necessary contingencies for posterior inference within it. Posterior inference involves comparing the plausibility of different beliefs (and their associated states). Non-parametric approaches allow the size of that state space to change.

While traditionally used in conditioning, state inference is becoming of interest for various psychiatric processes [@bedderModellingRuminationStateInference2023; @zikaTraitAnxietyAssociated2022]. The construction of multiple states and the dynamics between them builds on this notion in more complex environments [@hafnerLearningLatentDynamics2019]. Intervening on context has proven useful in various forms of therapy [@hitchcockComputationalPsychiatryNeeds2022] [@radulescuStateRepresentationMental2019a]. The importance of modelling inclusive of state inference has been shown in PTSD and rumination [@kubeRethinkingPosttraumaticStress2020] [@bedderModellingRuminationStateInference2023] [@lyndonHallucinationsPosttraumaticStress20200521].

Delusions can be explained well when newly considered states represent alternate explanations. A main belief (perhaps delusional), is held steadfast in the face of disconfirming sensory evidence. Instead of integrating the information of that evidence into the main belief through prediction error updating, its integration is allocated to an ad-hoc belief. This leaves the primary belief intact despite the presence of contradictory evidence, allowing the auxiliary hypothesis to protect the main hypothesis [@gershmanHowNeverBe2019]. This scenario has been used to describe delusions in a dirichlet process mixture model, in which a parameter affects the flexibility of whether of new 'beliefs' on which are made or old ones acted upon. [@erdmannGenerativeFrameworkStudy2022].

The combination of sequential state inference is understood in replay processes as essential for goal-directed decision making [@pezzuloInternallyGeneratedSequences2014], setting the stage for how state inference could become problematic. Beyond the current inferred state, but when inferring previous or future states relevant to the decision at the current timestep. In complex environments, where new connections between states, or new states must be inferred upon, we could instead interpret this new strategy as a new model. [@collinsCognitiveControlLearning20130128]. This incorporates non-parametric Bayesian statistics into analyses. In essence, the number of states must first be inferred before uncertainty over the state space is considered [@gershmanContextLearningExtinction20100111; @gershmanTutorialBayesianNonparametric2012; @archambeauNonparametricMixtureModeling2023]. The complexity of this approach is dangerous as each new states can lead to a necessity to re-cluster those states into possible trajectories.

\subsection{Contextual Inference}

Based on the 4 types of volatilities that we've described, what does context imply? Context could be used to differentiate value of the summary statistics at any level. Contextual inference could therefore refer to different level's summary statistics (representing the different levels of volatility). There may be undescribed effects of volatility attributable to contextual inference, making previous analyses incomplete on what level of volatility is being measured. Inference models could incorrectly ascribe volatility up the hierarchy if each level is not considered.

When multiple contexts models are considered, another consideration arises: distinguishing between variance within the context described, or variance that should be attributed to an alternate context [@healdContextualInferenceLearning2023; @findlingImpreciseNeuralComputations2020]. These types of volatility can be described as within and between context volatility, and single context models could misinterpret learning when contextual inference is taking place. This would result in an imprecise trajectory instead of multiple, precise ones.

Indicative of the need for structure learning in hierarchical models is the presence of improper state inference in many parts of schizophrenia. The logic is as follows: how can one calculate the uncertainty over expected values if there is uncertainty in the current state in the first place. Factoring in that higher level uncertainty (uncertainty over uncertainty) is sufficient for single state inference. However, problems can arise when considering that states are indicative of previous and future states, and incorrect state inference over one part of the sequence requires re-calculation of the entire trajectory. Contextual inference should therefore be thought of as the problem of multistability, extended to complex sequences [@hohwyPredictiveCodingExplains2008].

\section{How can structure learning be incorporated}\label{structure_learning}

Recall that we've identified a major shortcoming in the modelling approaches to understand symptoms in schizophrenia, particularly the development of delusions. The open question of 'where is the error' in a predictive hierarchy is difficult to answer, particularly through common inferential frameworks. A descriptive approach to the development of delusions would be hardcoding in dynamic priors. To better a predictive approach, a modeller can tie empirical data that may provide a mechanistic explanation to the dynamic learning rates underlying delusions. This would better explain how a compensatory mechanism (of strong high, weak low priors) could form.
Active inference solves the exploration / exploitation dilemma on an algorithmic level, and explanations to pathological behaviour is shown by a different prior and learning rate [@fristonActiveInferenceProcess2017a]. However it does not have a structured approach for incorporating changes in the state space model. This leaves open the difficulty of applying this framework to problems without a well defined, or changing, task structure. A principled approach that incorporates state inference in active inference models of schizophrenia is an opportunity to not only better describe this behaviour, but expose differing neural mechanisms.

The policy (or trajectory) in a dynamical system is based on the available sequence of possible state spaces, and accompanying uncertainty over them. Moving outside of the available policy space of the current model would constitute structure learning[@fristonSupervisedStructureLearning2023]. In active inference models, this has been conducted with Bayesian Model Reduction and Bayesian Model Expansion, in which nested or parent models, respectively, can be compared based on their accuracy and complexity. In machine learning and reinforcement learning, meta learning often describes the process of structure learning as 'learning to learn'. [@ortegaMetalearningSequentialStrategies2019c; @binzMetaLearnedModelsCognition2023]. The goal becomes to obtain inductive biases completely from the data. Or models can be selected between based on their own distribution [@finnProbabilisticModelAgnosticMetaLearning2018].

Structure learning can also be construed as occurring during a-ha moments, or periods where a new understanding of the volatility of the environment is made [@chenComputationalModelingEpiphany2017]. 
[@tulverRestructuringInsightIntegrative2023]. Different possible methods to include the advantages of structure learning in the inference models we've discussed are as follows: contextual inference, message passing, meta-models. We will hypothesize on connections of these to the better understand uncertainty, learning rates and state spaces.

\subsection{How to bridge inference models and structure learning}
The generative model approach in computational psychiatry allows shift theory driven understanding of descriptive models of volatility. Active inference remains poised to combine hypothesis driven and data driven approaches [@huysComputationalPsychiatryBridge2016]. In which, a generative model reduces the relevant parameters on which unsupervised approaches can better act upon. The universal approximator theorem argues that a sufficiently large neural network can approximate any nonlinear function with a finite set of parameters. This is helpful in computational bottlenecks that can arise in PGM's. The neural network model is then informed by the structured priors of the active inference model. Computational models need to balance the interpretability while exploiting the power of data driven technologies [@doerigNeuroconnectionistResearchProgramme2023].

Active inference inherently embodies generative models, and the dynamics are commonly studied as the learning of parameters. In an attempt to connect state space models to our neurobiological correlates, we want to understand how the state spaces of these models develop. The development of an active inference model can be broken up into inference (calculation of a posterior), learning (updating of parameters), and structure learning (updating the structure of the generative model). Through generative models that undergo active inference, we can better understand how parameters develop through first principles and simulate an agent's trajectory through a task. In contrast to computational parameters that are descriptive of the agents actions representing an average over the entire sequence of the task.

Active inference models are commonly represented through probabilistic graphical models (PGMs), while pairing them with neural networks can provide advantages. The probabilistic graphical model provides the a priori structure, for which parameters must maintain effective distributions, as they must reflect real world predictions. The PGM therefore situates the domain, and the neural network estimates the best way to describe it [@hallGINNsGraphInformedNeural2021]. This approach to modelling exploits a bridge between probablistic programming and differential programming. Techniques that use PPL to enable semi-parametric modeling, exploiting an inference engine that combines parametric and non-parametric models [@lavinDoublyBayesianOptimization2018] [@dalibardBOATBuildingAutoTuners2017] [@yangBPINNsBayesianPhysicsinformed2021].

Where to motivate priors should be ascribed to empirical data when possible, hence the motivation to include network models as 'empirical priors' [@fristonBayesianModelReduction2016]. Active inference models have previously been heuristically modified with machine learning methods. In some instances, these include replacing computationally intensive updates in the message passing being replaced by neural networks. This is known as 'inference compilation', and could be thought of an amortization of cost of computations of PPL models (Sajid, in press). Other adjacent fields can also be used as inspiration to field connections. These include reinforcement learning (Kumar et al., 2022; Binz & Schulz, 2022b; Jensen et al., 2023; Schubert et al., 2023), compositional reasoning (Jagadish et al., 2023; Lake & Baroni, 2023) and meta-learning [@bourginCognitiveModelPriors2019; @pattersonEmpiricalDesignReinforcement2023].

On the neuroscience side, active inference models have the appeal of integrating neural data analyses [@lindermanUsingComputationalTheory2017]. While the underlying statistical mechanics/theory of active inference form the usage of variational inference. This combines a hypothesis driven approach with room for data-driven latent state inference. [@lindermanUsingComputationalTheory2017]. The evolution of intrinsic dynamics has long been modeled with stochastic dynamic models, but It's not yet clear how to link these models with other a principled account of goal-directed action [@robertsClinicalApplicationsStochastic2017]. Machine learning approaches have also integrated agents allowed to actively change the state of their environment [@ueltzhofferDeepActiveInference2018b].

\subsection{Mixed models in active inference}

Discrete time models, while popularized by the POMDP format, are abstracted from the original continuous time formulation that centers the brain as a dynamical system. Hierarchical discrete state space models may capture a effect of increasingly abstract time as one goes up the hierarchy. The advantages of discrete models lies in the interpretability of states and their mapping to outcomes [@fristonSupervisedStructureLearning2023]. Also, there are computational advantages, such as extracting the the uncertainty of parameters, and the flexibility of the functional forms of posteriors and priors (as categorical or dirichlet distributions) makes inference problems more tractable.

Mixed models can also be used, where both discrete and continuous trajectories reduce variational free energy [@priorelliSlowFlexibleFast2023]. Traditionally, mixed models have been used with movement, where the movement level comprises of a generalized filtering agent, without expected free energy. A discrete level of the model enables goal-directed decision making under a free energy minimization cost function.

In a mixed hierarchical model, we can consider one level as discrete, actionable events that must be made. The input from a continuous level to a discrete can be configured through bayesian model reduction. Mixed models in active inference enable this delineation [@parrDiscreteContinuousBrain2018b]. Within the levels, the states already act as attractors, for which uncertainty guides the agents next step based on the free energy reduction [@fristonCognitiveDynamicsAttractors2014; @parrGenerativeModelsSequential2023]. Connecting attractors calculated with different cost functions is an open question, but future directions can link these neural network established state spaces to attractor dynamics in the brain [@khonaAttractorIntegratorNetworks2022].

Perhaps the most relevant for modifications with machine learning, a meta-modelling structure differs from a hierarchical one in what the objective and input of the system is, which determines the relevance of its cost function. In a hierarchical model, ascending levels take in the low levels as the input of its system. In a heterarchical structure, different models can take in input from eachother, but also from other sources. This means different models can differ in abstraction in a similar manner as hierarchical models, but be coupled to other sources of information (internal or environmental). Each structure takes in input from the others, building in inference processes that are attuned to multiple internal processes (as well as inference of external stimuli). The difference in which process, internal or external, can be captured by the influence of message passing between models, and their associated connections [@parrNeuronalMessagePassing2019] [@parrActiveDataSelection2024].

The open question is how to connect levels of different size state spaces, or provide unbiased linking functions. Work in machine learning is working on this connection using some common tools in cognitive science discussed here, such as bayesian nonparametrics to create new state spaces to act upon or using the evidence lower bound to cluster assignments [@schaefferBridgingAssociativeMemory2024]. The incorporation of both latent space analyses and goal-direction is similar to the variational auto-encoder, which uses a similar cost function.

\subsection{Message Passing}

Message passing amounts to the updating of a generative model by passing information to states through conditional dependencies. Bayesian inference often relies on a complete message passing, until the messages between opposing states are balanced. Variational message passing is often used in active inference, allowing several passes of information until the nodes 'settle' at an equilibrium. [@parrNeuronalMessagePassing2019]. In many sequential, single context tasks, this information is easy to settle due to the markov property of connected states. As state spaces are allowed to complexify, this process becomes computationally intense, likely drawing away from resource-rational principles and the plausibility of the process theory, or neural instantiation, of active inference.
Circular inference describes a process of aberrant message passing in hierarchical models, incomplete updating of the posterior associated with a latent state (Jardri et al., 2013). This approach offers a mechanistic answer to what allows these high priors to form, and further, what is the interaction between high prior and aberrant salience? [@jardriCircularInferencesSchizophrenia2013a]. Bistable perception has also been shown in circular inference, indicating that this approach could be further expanded to sequential inference [@leptourgosFunctionalTheoryBistable2020].

Another early and ongoing approach is reactive message passing, event-driven message passing that eliminates the need for pre-defined passing schemes. This approach can also be potentially trained on a neural network, to best adapt to the task. Conceptually, message passing can be connected to dysconnection in schizophrenia, where pruning of a model can be computationally specified by reducing the efficacy of defined parts of a structure. Message passing can be used to delineate recurrent and bottom-up feedback in a hierarchical or heterarchical model.

\subsection{Temporal Biases}

The effect of ordering that exposes an effect on later learning that is disrupted in schizophrenia, leading to problematic learning updates exposed through dynamic environments. Noise (and resulting changes) in dynamic environments is commonly assumed markovian, or uniformly random, in that the noise from one timestep does not influence the next timestep. Smoothened, or coloured noise, is temporally correlated and more realistic to environments and likely biological processes, as it accounts for temporal and spatial regularities. Information that changes over time is perceived to have aspects of autocorrelation within it. In essence, there is a need to handle environments that are dynamic, or for which there is time correlated (smooth, colored) noise. In stochastic processes such as sequential decision making environments, stationarity describes the independence of statistical properties across time [@gabbianiMathematicsNeuroscientists2017]. Environments which hold stationarity, and are assumed normally distributed can be described under white noise. This assumption is also baked into most models that do not incorporate generalized coordinates of motion.

While predictive coding models do implicitly handle time-series data already, the emphasis of these models has been on static stimuli, leaving open opportunities to include a time dimension [@millidgePredictiveCodingTheoretical2022a] [@millidgePredictiveCodingNetworks2024]. Generalized coordinates track higher order temporal motions of states, allowing a more precise understanding of environments with non-markovian noise (or non-static stimuli). This does provide short term smoothing over temporal dependencies, but is not suited for handling large environmental changes. (how do we handle this in terms of environments that are highly volatile, ). The autocovariance function of a stochastic process can be used to assess the weakness or strength of stationarity. Commonly used is a Fourier transform to assess the power spectrum. Considering that people may have unique interactions with non-gaussian noise, the laplace assumption may hinder our ability to build models that can properly help us understand pathological responses [@dacremontNeuralMechanismsIdentification2016].

\subsection{Switching Models}
A main problem in psychology is determining the strategy that an agent undertakes to determine actions. Some examples between strategy alternations are passive versus active, explorative vs exploitative, with heuristics or a complete model of the task [@ashwoodMiceAlternateDiscrete2022].

There is an ongoing effort to assess the set of rules or strategy that an agent uses on each trial [@maggiTrackingSubjectsStrategies2024]. [@royExtractingDynamicsBehavior2021; @ashwoodMiceAlternateDiscrete2022]. Temporally dependent interpretation of noise can be estimated through choice behaviour, so that switches between 'engaged' and 'random' choice patterns can be estimated [@liDynamicNoiseEstimation2024].

When needing to account for a total change in trajectories, it's easier & more simple to just consider a new set of available trajectories. While normally, adaptations to trajectories are made by changes in policies. The agent has several policies that they can use, which represent a best approximation of a trajectory over a state space that reduces the agents free energy + some stochasticity in that choice. So, given new information (perhaps regarding the precision over future information), one could imagine how an agent would switch between an 'aberrant salience' policy and then a 'delusional' policy. The difference between these two policies being the precision in information in which the agent continues on with over the rest of their trajectory. So it's essentially a switch between policies containing high precision over priors or weak.

Switching linear dynamical systems offer a framework that incorporates switces between state space trajectories [@lindermanBayesianLearningInference2017] [@hamakerRegimeSwitchingStateSpace2012] [@senozSwitchingHierarchicalGaussian2021]. Shifting between contexts to best exploit the statistical regularities within them may be a more parsimonious method [@wallConsistencyKeyLearning2023]. Shifting between policies could also explain all-or-nothing learning, as a switching between very high and very low learning rates. Measuring high level switches will rely on tasks that can shift the complexity of shifts between contexts (or periods of mean-shifts or oddballs) [@wallConsistencyKeyLearning2023].

However, it will still be difficult to account for deciding when to switch between contexts or policies (or measuring the 'between-context' uncertainty previously described). Possibly empirical priors for these switches in policy could be switches between the dominant brain network. Switches between brain states are problematic in schizophrenia[@northoffOvercomingRestTask2021]. Different brain networks are activated, and most pronounced for those with delusions, when incorrectly integrating evidence [@lavigneFunctionalBrainNetworks2020].

Dual process theories also describe this switching, and have been used for cognitive models. Cognitive flexibility similarly describes the ability to adapt an agent's behaviour depending on their level of control over their environment, which can be reflected via switches between habitual and goal-directed policies, and the cost associated for switching [@musslickExaminingCognitiveFlexibility2024a]. Task-switching in active inference models has been captured between states [@parrCognitiveEffortActive2023]. Deep neural networks are one way to modulate this flexibility in human learning [@sandbrinkModellingCognitiveFlexibility2024].

\subsection{Biophysical Models}
Unsupervised approaches to the construction of the generative model offer an alternate approach, but in turn, lose the ability to interpret meaningful parameters [@ueltzhofferDeepActiveInference2018b]. Biophysical models instead take advantage of the intrinsic structure and dynamics of neural activity. These dynamics can be used to govern the priors over the goal-directed agent. An active inference agent remains dictated by its goal of maintaining equilibrium with an external system through sensory (and active) interactions with the environment. The convergence of attractor models and predictive coding, but not yet active inference, models in this manner has been highlighted in regards to psychiatric disorders [@durstewitzPsychiatricIllnessesDisorders2021].

Perceptual and decision making behaviors have been captured via attractor models [@wangProbabilisticDecisionMaking2002, p. 200] [@bitzerBayesianAttractorModel2015]. Dynamical systems models in the brain can be used to measure an underlying flow of neural behaviours, and it is particularly suited for multistable switching and attracting dynamics [@robertsClinicalApplicationsStochastic2017, p. 201] [@robertsClinicalApplicationsStochastic2017a]. It is argued that stochastic dynamical models can provide a formalism for aberrant coding of the precision used in probabilistic formulations [@robertsClinicalApplicationsStochastic2017]. Stochastic models already incorporate inputs from the environment as noisy, providing the necessary tools for capturing the different types of uncertainty we want to dissect in schizophrenia . Noise itself moves the system through states, acting as the driver that separates the system into different attractors.

Cognitive control has also described the necessity to flexibility update behaviour in response to task demands. Prominent examples within schizophrenia have used dynamical systems models to explain the interaction between the prefrontal cortex and dopamine [@DurstewitzDualstateTheory]. Working memory models focused on the usefulness of attractor states from the differentiation of noise and useful information [@braunBrainNetworkDynamics2021]. In a similar manner, working memory models have used attractors to represent the dopaminergic modulation necessary to traverse switches between states, which could be readily applied to the between-context uncertainty discussed.
Several hypotheses posit that various symptoms in schizophrenia can be described by attractor landscapes. In thought disorder, problematic connections between disparate concepts are shown by a flattened landscape[@musaShallowCognitiveMap2022] [@rollsAttractorCorticalNeurodynamics2021; @lohDynamicalSystemsHypothesis2007]. The combination of relevant state spaces is explored in different ways, such as through context integration deficits are explored through [@calvinGlobalDisruptionExcitationinhibition2021]. Biophysically realistic models in schizophrenia already model working-memory processes as relating to gating. The effects of gating may also present in the MMN, and deficit affects extend beyond just pre-attentive processes [@shenP50N100P2002020; @boutrosSensoryGatingDeficits2004]. Sensory gating effects may also be controlled by top-down processes, evidenced by cortical changes responsible for sensory gating in response to attention [@golubicAttentionModulatesTopology2019].

Active inference models are built on the free energy principle, which relates biological systems as random dynamical systems, which through the reduction of free energy, approximate Bayesian inference [@fristonCognitiveDynamicsAttractors2014]. States for which an agent's trajectory expects act as attractors [@parrGenerativeModelsSequential2023]. Thus, the complex state-space models predicated by contextual inference could be dually interpreted as active inference and dynamical system models [@medranoLinkingFastSlow2024]
[@fristonFreeEnergyValue2011].

\section{Conclusion}\label{conclusion}

We consider the delusion paradox, the existence of high level, fixed beliefs despite a weak internal model, as an essential aspect of the phenomenology of schizophrenia. It's paradoxical existence can act as a window into the mechanism of learning and decision making to some extent, so a computational exploration of its biases is necessary. In this approach, we noted the capabilities of inferential models in describing this phenomenon. The fomentation and fomentation of a delusion must be viewed as a temporally evolving process in order to evaluate the evolution of these processes. A sequential decision making task gives certain advantages to explicitly exploring fundamentals of this delusion paradox. We've described active inference, and potential modifications, as a useful framework for exploring the topic from this lens.

The theoretical advantages of active inference in describing error in both a sequential and hierarchical manner were considered, are yet to be fully explored in the literature. Active inference's ability, through free energy and generalized coordinates, to describe uncertainty over the trajectory of a decision making sequence serves as an essential modelling tool going forward. The difficulties in describing uncertainty in a hierarchical manner are shown, motivating us to look at other learning biases that better explain the presence of the delusion paradox. Another burgeoning field, contextual inference, is well suited to describe uncertainties at different levels, and is more mathematically inclusive to shifts between these uncertainties. Finally, we posit how complicated applications of structure learning can fit within the active inference framework. Resource rational computational approaches, such as message passing, will motivate future approaches. Switching models will be motivated by empirical connections, and machine learning approaches and existing biophysical models also provide ready connections to active inference.

Maintaining the philosophical assumptions afforded by active inference models and the free energy principle is a motivating factor in this mixed model approach [@hohwySelfEvidencingBrain2016]. These include the motivations behind free-energy reduction by agents and agent-environment interactions as coupled systems, which are enacted through the maintenance of a generative model on which its predictive accuracy is continually updated. For those reasons, we must also subscribe to a hierarchical structure of the generative model. However, there is room for how that model is updated while maintaining the principles of free energy reduction.


\bibliographystyle{unsrtnat}
\bibliography{references}  
\end{document}
